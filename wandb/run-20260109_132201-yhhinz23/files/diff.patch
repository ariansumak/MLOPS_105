diff --git a/configs/train.yaml b/configs/train.yaml
index 5df5c1d..1a0d297 100644
--- a/configs/train.yaml
+++ b/configs/train.yaml
@@ -3,11 +3,10 @@ defaults:
   - _self_
 
 data:
-  train_dir: data/train
-  val_dir: data/val
-  test_dir: data/test
+  data_dir: /zhome/8a/1/224071/data/kaggle/chest-xray-pneumonia/chest_xray/
   batch_size: 16
   num_workers: 2
+  augment: true
 
 model:
   name: efficientnet_b0
diff --git a/pyproject.toml b/pyproject.toml
index ce5f47c..f180def 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -11,6 +11,7 @@ dependencies = [
     "pandas>=2.3.3",
     "torch>=2.9.1",
     "torchvision>=0.24.1",
+    "tqdm>=4.67.1",
     "typer>=0.21.1",
     "wandb>=0.23.1",
 ]
diff --git a/src/pneumoniaclassifier/evaluate.py b/src/pneumoniaclassifier/evaluate.py
index fe6f272..073cb1e 100644
--- a/src/pneumoniaclassifier/evaluate.py
+++ b/src/pneumoniaclassifier/evaluate.py
@@ -3,6 +3,7 @@ from __future__ import annotations
 import torch
 from torch import nn
 from torch.utils.data import DataLoader
+from tqdm import tqdm
 
 
 def evaluate(
@@ -10,6 +11,8 @@ def evaluate(
     loader: DataLoader,
     criterion: nn.Module,
     device: torch.device,
+    show_progress: bool = False,
+    description: str = "Validation",
 ) -> tuple[float, float]:
     """Evaluate a model on a validation dataset."""
 
@@ -19,7 +22,10 @@ def evaluate(
     total = 0
 
     with torch.inference_mode():
-        for inputs, targets in loader:
+        data_iter = loader
+        if show_progress:
+            data_iter = tqdm(loader, desc=description, dynamic_ncols=True, leave=False)
+        for inputs, targets in data_iter:
             inputs = inputs.to(device)
             targets = targets.to(device)
 
diff --git a/src/pneumoniaclassifier/train.py b/src/pneumoniaclassifier/train.py
index 94e89d6..8210dc5 100644
--- a/src/pneumoniaclassifier/train.py
+++ b/src/pneumoniaclassifier/train.py
@@ -6,14 +6,15 @@ from typing import Iterable
 
 import hydra
 import torch
+import wandb
 from hydra.core.config_store import ConfigStore
 from omegaconf import DictConfig, OmegaConf
 from torch import nn
 from torch.optim import Adam
-from torch.utils.data import DataLoader
 from torchvision import models
+from tqdm import tqdm
 
-from pneumoniaclassifier.data import MyDataset
+from pneumoniaclassifier.data import get_dataloaders
 from pneumoniaclassifier.evaluate import evaluate
 
 
@@ -21,11 +22,10 @@ from pneumoniaclassifier.evaluate import evaluate
 class DataConfig:
     """Configuration for dataset locations and loading."""
 
-    train_dir: Path = Path("data/train")
-    val_dir: Path = Path("data/val")
-    test_dir: Path = Path("data/test")
+    data_dir: Path = Path("/zhome/8a/1/224071/data/kaggle/chest-xray-pneumonia/chest_xray/")
     batch_size: int = 16
     num_workers: int = 2
+    augment: bool = True
 
 
 @dataclass
@@ -144,23 +144,6 @@ def _set_trainable_layers(model: nn.Module, unfreeze_blocks: int) -> None:
             param.requires_grad = True
 
 
-def _create_loader(
-    dataset: MyDataset,
-    batch_size: int,
-    num_workers: int,
-    shuffle: bool,
-) -> DataLoader:
-    """Create a dataloader with consistent settings."""
-
-    return DataLoader(
-        dataset,
-        batch_size=batch_size,
-        shuffle=shuffle,
-        num_workers=num_workers,
-        pin_memory=torch.cuda.is_available(),
-    )
-
-
 def _filter_trainable_parameters(model: nn.Module) -> Iterable[nn.Parameter]:
     """Return parameters that require gradients."""
 
@@ -173,8 +156,6 @@ def _init_wandb(config: TrainConfig) -> None:
     if not config.wandb.enabled:
         return
 
-    import wandb
-
     wandb.init(
         project=config.wandb.project,
         entity=config.wandb.entity,
@@ -194,20 +175,17 @@ def train(cfg: DictConfig) -> None:
     torch.manual_seed(train_config.train.seed)
     device = _get_device(train_config.train.device)
 
-    train_dataset = MyDataset(train_config.data.train_dir)
-    val_dataset = MyDataset(train_config.data.val_dir)
-
-    train_loader = _create_loader(
-        train_dataset,
+    print(f"Loading datasets from {train_config.data.data_dir}")
+    train_loader, val_loader, _ = get_dataloaders(
+        data_dir=str(train_config.data.data_dir),
         batch_size=train_config.data.batch_size,
         num_workers=train_config.data.num_workers,
-        shuffle=True,
+        augment=train_config.data.augment,
     )
-    val_loader = _create_loader(
-        val_dataset,
-        batch_size=train_config.data.batch_size,
-        num_workers=train_config.data.num_workers,
-        shuffle=False,
+    print(
+        f"Train samples: {len(train_loader.dataset)} | "
+        f"Val samples: {len(val_loader.dataset)} | "
+        f"Batch size: {train_config.data.batch_size}"
     )
 
     model = _build_model(
@@ -238,7 +216,13 @@ def train(cfg: DictConfig) -> None:
         epoch_correct = 0
         epoch_total = 0
 
-        for inputs, targets in train_loader:
+        train_progress = tqdm(
+            train_loader,
+            desc=f"Epoch {epoch}/{train_config.train.epochs} [train]",
+            dynamic_ncols=True,
+            leave=False,
+        )
+        for inputs, targets in train_progress:
             global_step += 1
             inputs = inputs.to(device)
             targets = targets.to(device)
@@ -254,6 +238,10 @@ def train(cfg: DictConfig) -> None:
             preds = outputs.argmax(dim=1)
             epoch_correct += (preds == targets).sum().item()
             epoch_total += batch_size
+            train_progress.set_postfix(
+                loss=f"{loss.item():.4f}",
+                acc=f"{(preds == targets).float().mean().item():.4f}",
+            )
 
             if (
                 train_config.train.log_interval_steps > 0
@@ -276,6 +264,7 @@ def train(cfg: DictConfig) -> None:
                 train_config.eval.interval_steps > 0
                 and global_step % train_config.eval.interval_steps == 0
             ):
+                print(f"Running validation at step {global_step}")
                 val_loss, val_acc = evaluate(model, val_loader, criterion, device)
                 if train_config.wandb.enabled:
                     import wandb
@@ -293,7 +282,14 @@ def train(cfg: DictConfig) -> None:
         epoch_acc = epoch_correct / max(epoch_total, 1)
 
         if train_config.eval.run_at_epoch_end:
-            val_loss, val_acc = evaluate(model, val_loader, criterion, device)
+            val_loss, val_acc = evaluate(
+                model,
+                val_loader,
+                criterion,
+                device,
+                show_progress=True,
+                description=f"Epoch {epoch}/{train_config.train.epochs} [val]",
+            )
         else:
             val_loss, val_acc = 0.0, 0.0
 
diff --git a/uv.lock b/uv.lock
index a122c99..02ad5a9 100644
--- a/uv.lock
+++ b/uv.lock
@@ -474,6 +474,7 @@ dependencies = [
     { name = "pandas" },
     { name = "torch" },
     { name = "torchvision" },
+    { name = "tqdm" },
     { name = "typer" },
     { name = "wandb" },
 ]
@@ -493,6 +494,7 @@ requires-dist = [
     { name = "pandas", specifier = ">=2.3.3" },
     { name = "torch", specifier = ">=2.9.1" },
     { name = "torchvision", specifier = ">=0.24.1" },
+    { name = "tqdm", specifier = ">=4.67.1" },
     { name = "typer", specifier = ">=0.21.1" },
     { name = "wandb", specifier = ">=0.23.1" },
 ]
@@ -1209,6 +1211,18 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/d6/ab/e2bcc7c2f13d882a58f8b30ff86f794210b075736587ea50f8c545834f8a/torchvision-0.24.1-cp314-cp314t-win_amd64.whl", hash = "sha256:480b271d6edff83ac2e8d69bbb4cf2073f93366516a50d48f140ccfceedb002e", size = 4335190, upload-time = "2025-11-12T15:25:35.745Z" },
 ]
 
+[[package]]
+name = "tqdm"
+version = "4.67.1"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "colorama", marker = "sys_platform == 'win32'" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/a8/4b/29b4ef32e036bb34e4ab51796dd745cdba7ed47ad142a9f4a1eb8e0c744d/tqdm-4.67.1.tar.gz", hash = "sha256:f8aef9c52c08c13a65f30ea34f4e5aac3fd1a34959879d7e59e63027286627f2", size = 169737, upload-time = "2024-11-24T20:12:22.481Z" }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl", hash = "sha256:26445eca388f82e72884e0d580d5464cd801a3ea01e63e5601bdff9ba6a48de2", size = 78540, upload-time = "2024-11-24T20:12:19.698Z" },
+]
+
 [[package]]
 name = "triton"
 version = "3.5.1"
